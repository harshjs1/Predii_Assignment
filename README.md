# Predii_Assignment

# ğŸš— Vehicle Specification Extraction using RAG (LangChain + Llama-3)

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline to extract structured vehicle specifications (e.g., torque values, part numbers, fluid capacities) from an automotive service manual PDF using **Llama-3 via Ollama**.

---

## âœ… Features
- ğŸ“„ PDF text extraction using **PyMuPDF**
- âœ‚ï¸ Text chunking using **LangChain text splitters**
- ğŸ” Semantic embeddings using **Sentence Transformers**
- ğŸ§  Vector storage & retrieval using **ChromaDB**
- ğŸ¤– Local LLM inference using **Llama-3 via Ollama**
- ğŸ“Š Output in structured **JSON format**

---

## ğŸ›  Tech Stack
- **Python**
- **LangChain**
- **PyMuPDF**
- **Sentence Transformers**
- **ChromaDB**
- **Ollama + Llama-3**

---

## ğŸ“‚ Project Workflow
1. **PDF Parsing** â€“ Extracts raw text from `/content/service_manual.pdf`.
2. **Chunking** â€“ Splits large text into overlapping chunks.
3. **Embedding** â€“ Converts chunks into vector embeddings.
4. **Vector Store** â€“ Stores embeddings in ChromaDB.
5. **RAG Pipeline** â€“ Retrieves relevant chunks for a query.
6. **LLM Inference** â€“ Llama-3 extracts structured specifications.
7. **JSON Output** â€“ Returns clean structured data.

---

## â–¶ï¸ How to Run
1. Clone the repository.
2. Open the notebook in **Google Colab or Jupyter**.
3. Upload your PDF as `/content/service_manual.pdf`.
4. Run cells **in order (Cell 1 â†’ Cell 8)**.
5. Enter your query in the final cell, for example:
