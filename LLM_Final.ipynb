{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vehicle Specification Extraction — Notebook README\n",
        "\n",
        "This notebook extracts vehicle specifications (torques, fluid capacities, part numbers, etc.) from a service manual PDF using:\n",
        "- PyMuPDF for robust PDF text extraction\n",
        "- LangChain text-splitting for chunking\n",
        "- HuggingFace Sentence-Transformers for embeddings\n",
        "- Chroma for vector storage & retrieval\n",
        "- Ollama + Llama-3 as the LLM (via `langchain-ollama`)\n",
        "\n",
        "Run the code cells in order (Cell 1 → Cell 8).  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qa61d59xrT_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install all Python packages needed for PDF parsing, embeddings, Chroma, and LangChain/Ollama.\n",
        "May take a few minutes — restart the runtime after a successful install.**\n"
      ],
      "metadata": {
        "id": "f3ZZh6XLrjKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_uBF0zHi1Jn"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain PyMuPDF chromadb sentence-transformers transformers huggingface_hub langchain-community langchain-ollama langchain-huggingface --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install and start the Ollama server, then pull the Llama-3 model for local inference.**\n"
      ],
      "metadata": {
        "id": "DrRcHxPosA6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "os.environ['PATH'] += os.pathsep + '/usr/local/bin'\n",
        "\n",
        "\n",
        "try:\n",
        "    subprocess.run(['ollama', 'list'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    print(\"Ollama server is already running.\")\n",
        "except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "    print(\"Starting Ollama server...\")\n",
        "\n",
        "    process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=os.setsid)\n",
        "\n",
        "    time.sleep(5)\n",
        "    print(\"Ollama server started.\")\n",
        "\n",
        "print(\"Pulling llama3:instruct model (this may take a few minutes)...\")\n",
        "!ollama pull llama3:instruct\n",
        "print(\"llama3:instruct model pulled successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX1Fppz2jWRm",
        "outputId": "056507df-5f83-43b5-fcc6-a187dd7d64ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Starting Ollama server...\n",
            "Ollama server started.\n",
            "Pulling llama3:instruct model (this may take a few minutes)...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "llama3:instruct model pulled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load /content/service_manual.pdf with PyMuPDF and extract all pages into one text string.\n",
        "Prints the number of characters extracted for a quick sanity check.**"
      ],
      "metadata": {
        "id": "_kRN6R08sRD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "pdf_path = \"/content/service_manual.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "text = \"\"\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "\n",
        "print(f\"Extracted {len(text)} characters from PDF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt5GmsJvjY7l",
        "outputId": "21ab03b9-8b2c-44c6-c991-05837fae2a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 856936 characters from PDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the large extracted text into overlapping chunks using LangChain’s splitter.\n",
        "Chunks keep context across boundaries and are sized for embedding.**"
      ],
      "metadata": {
        "id": "Y8ayYxqjsoPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents([Document(page_content=text)])\n",
        "print(f\"Created {len(chunks)} text chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOUC5DK3lZFx",
        "outputId": "2a2c4991-4fe4-4394-edf5-bc52065ef954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 1069 text chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the HuggingFace sentence-transformer embedding wrapper used for vectorization.\n",
        "Model: all-MiniLM-L6-v2 — fast and accurate for semantic search.**"
      ],
      "metadata": {
        "id": "_xjPa9XDtEwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TfM2fq9l2CR",
        "outputId": "f8866347-505b-4407-eac9-f6aa2821cd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Chroma vector store from the chunks and embeddings; persist it to disk.\n",
        "This enables fast similarity search for retrieval-augmented generation.**"
      ],
      "metadata": {
        "id": "Qv99-PPftOXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"chroma_db\"\n",
        "\n",
        "db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedder,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "\n",
        "print(\"ChromaDB vector store created and persisted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx4Vey9Rl5oz",
        "outputId": "4900c6b5-12b5-41f0-eb51-645d472f2d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChromaDB vector store created and persisted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the Llama-3 LLM (ChatOllama), connect the Chroma retriever, and build the RAG chain.\n",
        "qa_chain is ready to accept queries and return model responses.**"
      ],
      "metadata": {
        "id": "H-4oei9DtYLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "llm = ChatOllama(model=\"llama3:instruct\")\n",
        "\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the question based only on the following context:\\n{context}\\n\\nStrictly follow the output format instruction in the question.\"),\n",
        "    (\"user\", \"{question}\")\n",
        "])\n",
        "\n",
        "qa_chain = (\n",
        "    {\n",
        "        \"context\": retriever,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"✅ 'qa_chain' has been successfully defined. You can now run the next cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubjrQUTsl_KF",
        "outputId": "dadd45ec-2039-4037-a232-0ee3d72ebfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 'qa_chain' has been successfully defined. You can now run the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interactive query cell: Enter a question, run the RAG chain, and parse the model’s JSON output.\n",
        "Prints structured component, spec_type, value, unit (or raw output on parse failure).**"
      ],
      "metadata": {
        "id": "3Ov2iUq5tcFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "query = input(\"Enter your query (e.g., 'Torque for brake caliper bolts'): \")\n",
        "\n",
        "prompt = (\n",
        "    \"Extract the specification from the context and present the answer as JSON in this format:\\n\"\n",
        "    \"{\\n\"\n",
        "    \"  \\\"component\\\": \\\"...\\\",\\n\"\n",
        "    \"  \\\"spec_type\\\": \\\"...\\\",\\n\"\n",
        "    \"  \\\"value\\\": \\\"...\\\",\\n\"\n",
        "    \"  \\\"unit\\\": \\\"...\\\"\\n\"\n",
        "    \"}\\n\\n\"\n",
        "    f\"Question: {query}\"\n",
        ")\n",
        "\n",
        "response = qa_chain.invoke(prompt)\n",
        "\n",
        "\n",
        "try:\n",
        "    spec_json = json.loads(response)\n",
        "    print(\"\\nStructured Output (Parsed):\")\n",
        "    print(json.dumps(spec_json, indent=2))\n",
        "except json.JSONDecodeError:\n",
        "    print(\"\\n⚠️ Response was not valid JSON. Here's what the model returned:\")\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQk-7u_WmBxh",
        "outputId": "114873c8-2e51-4159-d82c-2e78ab2929a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (e.g., 'Torque for brake caliper bolts'): Torque for brake caliper bolt\n",
            "\n",
            "⚠️ Response was not valid JSON. Here's what the model returned:\n",
            "Based on the provided context, the torque specification for the brake caliper bolt is:\n",
            "\n",
            "{\n",
            "  \"component\": \"Brake flexible hose bracket bolt\",\n",
            "  \"spec_type\": \"Torque\",\n",
            "  \"value\": 30,\n",
            "  \"unit\": \"Nm\"\n",
            "}\n",
            "\n",
            "Note that there may be additional specifications or variations depending on the specific application or model.\n"
          ]
        }
      ]
    }
  ]
}